---
title: "BCO7007 AI&ML | Assessment3"
author: "Andrea Botes and Mohammad Aziz"
date: '2022-06-03'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
###Executive Summary
This report revolves around the Luna coin which is a crypto-currency coin that crashed in May of 2022. The main objective of this business report is to collect data by conducting data mining of the hashtag "luna" from Twitter which is a social media platform. By using "rtweet" and the subordination of other [R packages] we are able to explore the dataset, tidy or clean it, visualize the results, and finally conduct a sentiment analysis and develop an RMSE supervised model to predict how the Luna coin affected investors and users.

###Introduction
This report aims to outline the reaction of users and investors of the Luna coin after the massive market crash where, according to [bloomberg](https://www.bloomberg.com/graphics/2022-crypto-luna-terra-stablecoin-explainer/), "$60 billion of in Terra Coins went up in Algorithmic smoke" and some $600 billion in value was wiped out in terms of crypto market capitalization as reported in [the sun news online](https://www.thesun.co.uk/money/18551885/cryptocurrency-market-crash-bitcoin-ethereum-luna/). Luna coin is a crypto-currency algorithmic coin that could possibly be subsidised by its sister token, UST, as UST is meant to be traded at 1:1 to fiat US$. "One UST can always be exchanged for a floating quantity of Luna with a market value of $1. Whenever either of them is exchanged, the currency is taken out of the circulation by the smart contracts programmed to maintain the system." (Shen Muyao, 2022)

The rypto currency market is an extremely high-risk market for investors. This statement is supported by [Money Smart](https://moneysmart.gov.au/investment-warnings/cryptocurrencies) who stated that the "price of crypto-assets can fluctuate at extreme levels based solely on market speculation. Factors that can influence the price of crypto include:

- media focus
- public announcements
- the actions of individuals who hold large amounts of a crypto or who influence the price through social media"

This report aims to collect data by conducting data mining on the hashtag "luna" from social media giant Twitter by using "rtweet" and the subordination of other [R packages] we are able to explore the dataset, tidy or clean it, visualize the results and finally, conduct a sentiment analysis and develop an RMSE supervised model to predict how the Luna coin affected investors and users. 

The first step consisted of mining or collecting the data associated to the hashtag, after which we analysed the tweets withing the dataset, which in turn allowed us to visualise the data by creating wordcloud and graphs. After the inital collecting and cleaning of the data we could subsequently conduct a sentiment analysis of the tweets using the "bing" lexicon to determine what positive and negative words are used within the tweets made by users. This was followed with LDA modelling and applying a RSME supervised model on the data. The body of the report will provide an illustration and functionality of the codes used.

```{r}
#The lines of code below load the required libraries:
library(rtweet)
library(tidyverse)
library(tidytext)
library(knitr)
library(readr)
library(tm)
library(ggplot2)
library(tidyr)
library(tidymodels)
library(emo)
library(textdata)

#These libraries were used to load the functions:
library(recipes)
library(rsample)
library(parsnip)
library(tune)
library(dials)
library(workflows)
library(yardstick)
library(janitor)
library(stringr)
library(xgboost)
library(randomForest)
```

#Note: Line 31 - 47 should not be runned as part of this report. It is for demonstration only!

```{r}
#Retrieving tweets and retweets containing the hashtag Luna.
luna_tweets <- search_tweets(
  q="luna",
  n=20,
  include_rts = TRUE,
  lang="en",
  retryonratelimit = TRUE
)

#Coverting the dataset to a short and cleaned dataset for more useful exploration. 
luna_short<-luna_tweets%>%
  select(user_id, screen_name, created_at, text, favorite_count, quoted_location, is_retweet, retweet_count, favorite_count, hashtags, mentions_user_id, mentions_screen_name, quoted_followers_count)

#Writing the retrieved dataset in a csv format
luna_short%>%write_csv("luna_short_08_06_2022.csv")
```

#The above chunks of codes (line 31 - 47) allowed us to mine the tweets and retweets containing the keyword 'luna' and assigning it to 'luna_tweets' as an object name. Out of the 90 variables and ~15500 observations, only 13 variables that were relevant to our work was selected from the object name and re-assigned to the 'luna_short' object name. We were then finally able to write the dataset to a csv formated file, namely 'luna_short_08_06_2022.csv', and save it to a directory which will henceforth be used as our saved data set.

```{r}
#Read data set from csv and saved it to a variable name (e.g. luna_tweets)
luna_tweets <- read_csv("luna_short_08_06_2022.csv")

#Provided an overview of the dataset with data types.
glimpse(luna_tweets)
```

```{r}
#Apply "str()" function to display the internal structure of data set. 
str(luna_tweets)

#Get head(first rows) of the data frame and assign it to an object name (d1).
d1 = head(luna_tweets)

#Display earlier saved object with "kable" in an "html" format for better visibility.

kable(d1, format = "html")

```

```{r}
#Plotting the frequency of the tweets for the current data set.
ts_plot(luna_tweets, "minutes", trim = 1L) +
  labs(x = NULL, y = NULL,
       title = "Frequency of tweets with luna",
       subtitle = paste0(format(min(luna_tweets$created_at), "%d %B %Y"), " to ", format(max(luna_tweets$created_at),"%d %B %Y")),
       caption = "luna tweets Data collected from Twitter") +
  geom_point() +
  geom_smooth(se = F, color = "cadetblue") +
  scale_color_brewer() +
  theme_light()+
  ggplot2::theme(
    legend.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(face = "bold")
  )
```

#Time series plot above indicates how luna attracts Twitter users from ~16:00 June 07 2022, then the slope leans downward by ~ 21:00, after which there is an exponential growth in luna hashtag users after 21:00.

```{r}
#The chunk of code below identifies the most retweeted accounts by filtering the retweets and then filtering the top 5 using 'head()' function. Based on results [screen_name] 'Luna_Selene9' reserved the top of the list with 252338 retweet_count.
luna_tweets%>%
  select(screen_name, text, retweet_count, is_retweet)%>%
  filter(is_retweet == TRUE)%>%
  select(screen_name, retweet_count)%>%
  arrange(desc(retweet_count))%>%
  head(5)

```

```{r}
#In the chunk of code below the top "tweeting locations" are retrieved. Based on results 'â™¡I love you so much my Maruâ™¡' was identified as a location which, by suprise, we couldn't locate as a location on the map!
luna_tweets%>%
  select(quoted_location, is_retweet)%>%
  filter(!is.na(quoted_location), quoted_location != "Ã˜", quoted_location != "", is_retweet == FALSE)%>%
  count(quoted_location, sort = T)%>%
  arrange(desc(n))%>%
  head(5)
```

```{r}
#Obtain time series of Twitter name 'luna' and assign it to 'lunatime' object.
lunatime <- get_timelines("luna", n= 3200)

#See summary of tweets with 'luna' Twitter account from when it starts untill the last tweet.

lunatime %>%
  summarise(min(created_at), max(created_at))

#visualize the tweets by week time series.
lunatime %>%
  filter(created_at >= "2011-09-28") %>%
  group_by(screen_name) %>%
  ts_plot("weeks", trim = 1L) +
  geom_point() +
  geom_smooth(se = F, color = "cadetblue") +
  scale_color_brewer() +
  theme_light()+
  ggplot2::theme(
    legend.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(face = "bold")
  ) +
  labs(
    x = NULL, y = NULL,
    title = "Frequency of Twitter statuses",
  )
```
#Our analysis shows that users' privacy plays a vital role in data mining. For instance,  we wanted to determine when and how active "Dokwon", the CEO and founder of UST, and "cz_binance", COE and founder of Binance exchange and a large shareholder/co-founder of UST, are but both users deny data mining to occur on their accounts. Instead, we conducted a time series data mining on  the user name 'luna', which indicates that the users are quite active between 2011 and 2012 but the activity slowly dies down, and, by late 2017 there is no sign of activity of the specific user name.

```{r}

#the below chunks of code extract emojis from the 'text' variable of our data set, counts the use of every emoji, and then filters the top 10 most used ones. The most used emoji 'ðŸ”¥' has a count of 1162.

luna_tweets %>%
  mutate(emoji = ji_extract_all(text)) %>%
  unnest(cols = c(emoji)) %>%
  count(emoji, sort = TRUE) %>%
  top_n(10)
```

```{r}
#In the below chunk of code we have extracted the most common hashtags that were used alongside #luna in the 'text' variable, then applied the coundtfunction, sorted them in descending order, and finally displayed the top 10 hashtaqs. It appears that "Luna" in the form of different typos such as "luna/LUNA/Luna" is the most used: around 3232 times within the data set.

luna_tweets %>% 
  unnest_tokens(hashtag, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(hashtag, "^#"),
        hashtag != "#ClimateEmergency") %>%
  count(hashtag, sort = TRUE) %>%
  top_n(10)

```

##Hereafter we are going to tidy our dataset further and prepare it for the sentiment analysis, LDA modelling and developing a root mean square deviation model.

```{r}
#Lines 196 to 204 are used to tidy up our text further (e.g. filter out NA, remove "https"...)
tidy_luna <- luna_tweets %>%
  mutate(text) %>%
  unnest_tokens(word , text) %>%
  anti_join(get_stopwords()) %>%
  filter(!is.na(word) & !grepl("[^A-Za-z]" , word) & word != "https" & word !="na" & word != "trump" & word != "one")

#Word count of the variable "word" and sort Z - A on "n".
tidy_luna %>%
  count(word, sort = TRUE)
```

```{r}
#Get the dataset and apply anti_join on the stop_words function and assign it to "clean_tidy_luna" for further work.
clean_tidy_luna <- tidy_luna %>%
  anti_join(stop_words)
```
#In lines 172 - 176 we mutated the 'text' variable and changed each word to a single observation after which we were able to filter un-wanted words out as well as remove NAs from the dataset. Lines 178 - 179 then does a count of the words which sorts them from Z - A based on 'n'. The 218757 observations decrease to 178188 as we save/assign our dataset to 'clean_tidy_luna' an an object name.

#We purposefully removed words ("trump", "https" and "one") from our dataset as this may later on cause issues with in the sentiment analysis and wordcloud.
```{r}

#Create a column plot of 10 most used words on 'word' variable.
clean_tidy_luna %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word , y= n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme_classic() +
  labs(x = "Count" , 
       y = "Unique words" , 
       title = "Unique word counts found in '#luna' Tweets")
```

#Ploting the top word count shown [luna, lunc, crypto, blue, senior, running, america, advisor, eric, democratic, administration] is slightly different to our word count order in lines 179 - 180, [luna, lunc, now, help, crypto, keep, name, following, blue, senior]. Upon further investigation it clearly indicates that with the use of the "anti_join(word_stop)" function we are able to further narrow down the dataset to 17129 observations.

#Now we have a clear view of our dataset: how it looks, what it is and what we want to do with it. We are now able to do a sentiment analysis of the dataset to see how Luna and other crypto investors and users have reacted to the market crash.

#The sentiment analysis is based on already built package and lexicons - different lexicons react to the dataset in different ways. By using "bing" we can remove popular lexicons or vocabulary of words (e.g. Bing, NRC and Afinn) below for the report. According to [kaggle](https://www.kaggle.com/datasets/andradaolteanu/bing-nrc-afinn-lexicons), Bing is a dictionary of words that segregate words into positive or negative feelings. NRC gives us an analysis of the words based on (positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and Afinn work differently to both NRC and Bing, which provide values (from -5 to +5, showing the intensity of the sentiment from negative to positive).

```{r}

#Get a vector of sentiments so to use for sentiment analysis. 
get_sentiments("bing")


#Apply and use "bing" sentiment out of different types sentiments.
bing_luna = clean_tidy_luna %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

#Plot the positive or negative words used in "review" variable of the dataset and see top 20 words of each.
bing_luna %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(title = "Top 10 Most frequent Positive/Negative words", 
       y = NULL, 
       x = NULL) +
  coord_flip() + theme_bw()
```

#A wordcloud is another great way to see the most used words in a book, topic or number of topics at a glance. Below word cloud shows the 20 most used words in our dataset.

```{r}
#Create a Corpus of the 'clean_tidy_luna' object we already created - Corpus is a database of words created here.
luna_corpus <- Corpus(VectorSource(clean_tidy_luna))

#Lines 245 - 249 clean up the data further (e.g. numbers, punctuations, whitespaces)
luna_corpus <- tm_map(luna_corpus, content_transformer(tolower))
luna_corpus <- tm_map(luna_corpus, removeNumbers)
luna_corpus <- tm_map(luna_corpus, removePunctuation, preserve_intra_word_dashes = FALSE)
luna_corpus <- tm_map(luna_corpus, removeWords, c("the", "true", "false", "and", stopwords("english")))
luna_corpus <-  tm_map(luna_corpus, stripWhitespace)



#Load library
library(RColorBrewer)

#Load library
library(wordcloud)

#Wordcloud of the document is created to see what the most repeated words are.
wordcloud(luna_corpus, min.freq=2, max.words = 50, random.order = F, colors = brewer.pal(8, "Dark2"))

```

#According to [rstudio pubs](https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html) "Linear discriminant analysis (LDA) and the related Fisherâ€™s linear discriminant are methods used in statistics, pattern recognition and machine learning to find a linear combination of features which characterizes or separates two or more classes of objects or events". For the purpose of this report, with the use of LDA we can distribute the words into as many topics as we like and then analyse which word is most used in different topics. An alternative approach: we can get many topics and then analyse the words used within a different topic.

```{r}

#Create a DTM matrix of the data set to prepare it for modeling.
dtm<-DocumentTermMatrix(luna_corpus)

#Remove the zero values that may generate glitch while running the code.
raw.sum=apply(dtm,1,FUN=sum)
dtm=dtm[raw.sum!=0,]

#Create random numbers
set.seed(2022)

#Load library to use LDA
library(topicmodels)

#Create LDA model of the data set. here we use 'k=2' just to create two topic or a two dimension topic.
model_lda <- LDA(dtm, k=2, control = list(seed = 1234))
model_lda

#Tidy with beta matrix shows an over view of which word in which topic and 'beta' overview of the words in topics and topics' words, and show them as which word is used in which topic. We can also conduct and alpha matrix as we which.
#extraxt beta of the LDA model
beta_topic <- tidy(model_lda, matrix = "beta")
beta_topic

```


```{r}

#Prepare data set for visualization.
beta_top_term <- beta_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)


#Visualisation of the topic.
beta_top_term %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free") + 
  scale_y_reordered()

#words are randomly selected and assigned to different topics. Green and red here, unlike the sentiment analysis, does not represent positivity or negativity.
```


```{r}

#For the purpose of modeling, we will only use "favorite_count,is_retweet, retweet_count" as we want to predict if retweets and the number of retweets affect the number of tweet likes or not. For this assignment we are going to use "xg_boost" modeling.

#Lines 320 to 327 fetch the dataset that we read and saved in line 55, select a number of required variables, mutate 'is_retweet' to binary numbers and save it to the object name "luna_rec" which will then be further used for the modeling recipe and moving forward.

luna_rec <- luna_tweets %>%
  select(user_id, favorite_count,is_retweet, retweet_count) %>%
  mutate(
    is_retweet = case_when(
      is_retweet == "FALSE" ~ 1,
      is_retweet == "TRUE" ~ 0
    )
  )
```

```{r}
#Call random numbers
set.seed(2022)

#Split the dataset to two different portions (e.g. 70 percent for training and 30 percent for testing) and randomly select observation for the split.

data_split <- sample(2, nrow(luna_tweets), replace = T, prob = c(0.70, 0.30))

#Allocate the first split the 'data_split' to training and call it 'train'.
train <- luna_tweets[data_split == 1,]

#Allocate the second split the 'data_split' to testing and call it 'test'.
test <- luna_tweets[data_split==2,]

```

```{r}
library("gbm")
library(MASS)

#Boosted trees is a commonly used regression. In order to create the basic boosted tree model, we use 'gbm'.

model = gbm(is_retweet ~ retweet_count + favorite_count, data = train)

model

#Load library for ridyge regression use.
library(caret)

#Create random numbers
set.seed(2022)

#Train model
model <- train(favorite_count ~ is_retweet + retweet_count,
  data = train,
  method = "gbm",
  verbose = F
)

model

#Plot model to see insights
plot(model)

#Save plot of model to object 'm1' for later use
m1 = plot(model)

#Random number generator
set.seed(2022)

#Create a 10 fold sample using 'cv' method with train control
ctr <- trainControl(
  method = "cv",
  number = 10
)

#Create the second model
model2 <- train(
   retweet_count ~ favorite_count + is_retweet,
  data = train,
  method = "gbm",
  preProcess = c("center", "scale"),
  trControl = ctr,
  verbose = FALSE
)

model2

#Plot model
plot(model2)

#Save model plot to a variable name
m2 = plot(model2)
```

```{r}
#Plot the results of the models and see if there is any different between the models created.
library(ggpubr)
ggarrange(m1, m2, ncol = 2, nrow = 1)
```
##Conclusion

Our finding on this particular dataset indicates that the correlation between a tweet getting retweeted or whether the number of retweets is very high, does not necessarily represent the likelihood of tweets being liked by many real Twitter users, particularly crypto and Luna investors and users in our case.

Lines 109-114 indicates that most of the top Twitter names that get retweeted and liked are not real people's names, we can speculate that too many fake accounts and bots are involved in the tweet activity around the topic. When it comes to the crypto-currency market, particularly in the event of the UST-Luna market crash, bots and fake accounts' activity is much higher in terms of retweeting, liking and commenting on tweets.

The crypto-currency market is an unregulated market which means that the risks of scamming and crashes are highly likely in comparison to other trading markets. Investors must be very cautious of investing large capital as the crypto-currency market hits 3 trillion and the number of investors grow as there is a need for governments to step in and introduce regulations.

##Reference:
Shen, M 2022, How $60Billion in Terra Coins Went Up in Alghorithmic Smoke, Bloomberg, viewed 7 June 2022, <https://www.bloomberg.com/graphics/2022-crypto-luna-terra-stablecoin-explainer>.

Duffy, J 2022, Out of Cash, The Sun, viewed 8 June 2022, <https://www.thesun.co.uk/money/18551885/cryptocurrency-market-crash-bitcoin-ethereum-luna>.

Money Smart 2022, Cryptocurrencies: The risks of investing in crypto, Money Smart, viewed 10 June 2022, <https://moneysmart.gov.au/investment-warnings/cryptocurrencies>.

Olteanu, A n.d., Bing, NRC, Afinn Lexicons, Kaggle, viewed 8 June 2022, <https://www.kaggle.com/datasets/andradaolteanu/bing-nrc-afinn-lexicons>.

Martos, G n.d., Dsicriminant Analysis in R, Rstudio Pubs, viewed 10 June 2022 <https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html>.